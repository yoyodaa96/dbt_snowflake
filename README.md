**ğŸš€ Pipeline Data Moderne avec Snowflake, dbt et Apache Airflow
ğŸ¯ Objectif du projet*

Ce projet illustre la mise en place dâ€™un pipeline de donnÃ©es moderne de bout en bout.
Lâ€™objectif Ã©tait de transformer des donnÃ©es brutes issues de fichiers CSV (commandes, clients et livraisons) en informations exploitables au sein de Snowflake, Ã  lâ€™aide de dbt pour la transformation et Apache Airflow pour lâ€™orchestration automatisÃ©e.

ğŸ§© Architecture globale

Le flux de donnÃ©es suit une architecture ELT (Extract, Load, Transform) :

Les fichiers CSV sont chargÃ©s dans Snowflake, dans un schÃ©ma â€œRAWâ€.

Les modÃ¨les dbt effectuent les transformations et alimentent un schÃ©ma â€œANALYTICSâ€.

Apache Airflow orchestre et automatise lâ€™exÃ©cution quotidienne du pipeline.

En rÃ©sumÃ© :
CSV â†’ Snowflake (RAW) â†’ dbt (Transformations) â†’ Snowflake (ANALYTICS) â†’ Airflow (Automation)

â„ï¸ Snowflake

Snowflake hÃ©berge les donnÃ©es tout au long du cycle :

Le schÃ©ma RAW stocke les donnÃ©es brutes importÃ©es depuis les CSV.

Le schÃ©ma ANALYTICS contient les vues finales prÃªtes pour lâ€™analyse, issues des modÃ¨les dbt.

Les tables concernent les entitÃ©s principales : clients, commandes et livraisons.

ğŸ§± dbt (Data Build Tool)

dbt assure la transformation des donnÃ©es avec une logique SQL modulaire et rÃ©utilisable.
Les modÃ¨les de transformation relient les commandes aux clients et aux livraisons pour dÃ©tecter notamment les commandes en retard.
Le rÃ©sultat est une vue analytique claire dans Snowflake, actualisable Ã  chaque exÃ©cution du pipeline.

ğŸ•“ Apache Airflow

Airflow automatise et planifie les exÃ©cutions de dbt.
Chaque jour, un DAG dÃ©clenche les modÃ¨les dbt, contrÃ´le leur exÃ©cution et consigne lâ€™Ã©tat du pipeline.
Cette Ã©tape permet dâ€™obtenir un pipeline entiÃ¨rement automatisÃ© et supervisÃ©.

ğŸ“Š RÃ©sultat

Ã€ la fin de chaque exÃ©cution :

Les donnÃ©es brutes sont transformÃ©es en indicateurs prÃªts Ã  lâ€™emploi.

Les retards de livraison sont dÃ©tectÃ©s automatiquement.

Les transformations sâ€™exÃ©cutent de maniÃ¨re planifiÃ©e sans intervention manuelle.

Ce projet montre concrÃ¨tement comment combiner Snowflake, dbt et Airflow pour crÃ©er un pipeline scalable, maintenable et automatisÃ©, parfaitement reprÃ©sentatif dâ€™un environnement data moderne.
